name: dpo_qwen_experiment
project: DPO
workspace: Bijan

environment:
    image: bvarjavandhpe/dpo:0.1

hyperparameters:
  model_name: Qwen/Qwen2-0.5B
  dataset_name: tiny_shakespeare
  beta: 0.1
  max_length: 128
  num_workers: 2
  training_args:
    output_dir: "/tmp/llm_finetuning"
    max_steps: 5000
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    fp16: true
    evaluation_strategy: "steps"
    eval_steps: 1000
    logging_strategy: "steps"
    logging_steps: 100
    save_strategy: "steps"
    save_steps: 1000
    learning_rate: 2e-5


resources:
  resource_pool: A100
  slots_per_trial: 1  # Adjust this based on your available GPUs

entrypoint: model_def:DPOTrial

searcher:
  name: single
  metric: validation_loss
  smaller_is_better: true
  max_length:
    batches: 1000

max_restarts: 0