name: dpo_qwen_experiment
project: DPO
workspace: Bijan

debug: false
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
  image: bvarjavandhpe/dpo:0.1

hyperparameters:
  model_name: Qwen/Qwen2-0.5B
  dataset_name: tiny_shakespeare
  beta: 0.1
  max_length: 128
  num_workers: 1
  training_args:
    output_dir: "/tmp/llm_finetuning"
    max_steps: 5000
    per_device_train_batch_size: 1 # 8
    per_device_eval_batch_size: 4
    fp16: true
    evaluation_strategy: "steps"
    eval_steps: 1000
    logging_strategy: "steps"
    logging_steps: 100
    save_strategy: "steps"
    save_steps: 1000
    learning_rate: 2e-5


resources:
  resource_pool: A100
  slots_per_trial: 1  # Adjust this based on your available GPUs

entrypoint: >-
  python -m determined.launch.torch_distributed
  python finetune.py

searcher:
  name: single
  metric: validation_loss
  smaller_is_better: true
  max_length:
    batches: 1000

max_restarts: 0